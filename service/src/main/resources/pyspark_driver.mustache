{{=<% %>=}}

"""
Spark streaming job that monitors a specified kafka topic for opc json and performs dot product

Usage: dotproduct.py <zk> <topic>
To run locally, you must start kafka, create a topic, and produce opc json
Example:
$SPARK_HOME/bin/spark-submit --jars $SPARK_HOME/external/kafka-assembly/\
spark-streaming-kafka-assembly_2.10-1.3.1.jar dotproduct.py localhost:2181 runtime
"""

from __future__ import print_function

__author__ = 'jkidd'

import sys
import json
import numpy as np
import requests

from pyspark import SparkContext
from pyspark.streaming import StreamingContext
from pyspark.streaming.kafka import KafkaUtils

# temporarily hardcode url and write tag
url = "http://172.16.167.130:8000/updatedata/InferredCalc1"
write_tag = "PICK_INFER_MEAS/INFER1.CV"

def evaluateRDD(rdd):
    if (rdd.isEmpty() == False):
        rdd.foreach(evaluate)

def evaluate(tags):
    vector = [float(tags[tag]["Val"]) for tag in tags]
    scalar = np.dot(vector, [1, 1, 1, 1])
    writeOPCValue(scalar)

def writeOPCValue(value):
    payload = "%s,%.3f" % (write_tag,value)
    print(payload)
    requests.post(url, data=payload, headers={'content-type': 'text/plain'})

if __name__ == "__main__":

    # verify the input arguments
    if len(sys.argv) != 3:
        print("Usage: dotproduct.py <zk> <topic>", file=sys.stderr)
        exit(-1)

    # create a spark context and a streaming context
    sc = SparkContext(appName="OPCKafkaConsumer")
    ssc = StreamingContext(sc, 1)

    zkQuorum, topic = sys.argv[1:]
    input = KafkaUtils.createStream(ssc, zkQuorum, "opc-kafka-consumer", {topic: 1})

    input.map(lambda x: json.loads(x[1].decode("utf-8")))\
        .foreachRDD(evaluateRDD)

    ssc.start()
    ssc.awaitTermination()